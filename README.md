      
<h1 align="center">
    <strong>Awesome prompt hacking – an awesome list of curated resources on prompt hacking and AI safety.</strong>
</h1>

<p align="center">
Topics include AI red teaming, jailbreaking, prompt injection, prompt hacking, AI/ML safety and security.
</p>

<p align="center">
      
  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Made With Love](https://img.shields.io/badge/Made%20With-Love-orange.svg)](https://github.com/chetanraj/awesome-github-badges) [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Awesome%20Prompting%20Resources%20-%20a%20collection%20of%20awesome%20lists%20related%20to%20prompt%20engineering%20by%20@learnprompting&url=https://github.com/kavaivaleri/prompting-resources)
</p>

> This resource is provided by [Learn Prompting](https://github.com/trigaten/Learn_Prompting), your go-to resource for mastering Generative AI.

<img src="astronaut-learn-prompting.png" alt="astronaut-learn-prompting">

<p align="center">
<a href="https://discord.com/invite/learn-prompting-1046228027434086460">Discord</a> • <a href="https://x.com/learnprompting">Twitter (X)</a> • <a href="https://www.linkedin.com/company/learn-prompting/">LinkedIn</a> • <a href="https://learnprompting.beehiiv.com/subscribe">Newsletter</a> • <a href="https://learnprompting.org/courses/intro-to-prompt-hacking">Free Intro Prompt Hacking Course</a> • <a href="https://learnprompting.org/courses/advanced-prompt-hacking">Free Advanced Prompt Hacking Course</a>

</p>

---

## Table of Contents

- [Introduction](#introduction)
- Resource Categories
  - [Blogs](#blogs)
  - [Communities](#communities)
  - [Courses](#courses)
  - [Events](#events)
  - [Jailbreaks](#jailbreaks)
  - [YouTube](#youtube)
- [Contributing](#contributing)

---

## Introduction
Prompt Hacking is an emerging field that covers the intersection between AI and Cybersecurity. It involves exploring the outer edges of LLM behavior through adversarial prompts and prompt injection techniques.

Due to its novelty, online resources are few and far between.

This repository aims to provide a good overview of materials and tutorials that help expose vulnerabilities, document offensive research, and promote a better understanding of model limitations.

---

## Blogs

Stay informed with expert analyses, tutorials, and research articles on AI security.

- **[InjectPrompt](https://injectprompt.com)** – A comprehensive catalogue of novel jailbreaks, prompt injections, and system prompt leaks.
- **[LearnPrompting Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/introduction)** – A step-by-step guide on prompt injection and model exploitation.
- **[AIBlade](https://www.aiblade.net/)** – Directory of AI red teaming tools and methodologies.
- **[EmbraceTheRed](https://embracethered.com/blog/)** – Practical experiments and insights from active AI red teamers.
- **[Joseph Thacker](https://josephthacker.com/)** – First-person narratives on red teaming and LLM vulnerability research.
- **[Protect AI Blog](https://protectai.com/blog)** – Enterprise insights on AI security along with open-source tooling.
- **[AWS Generative AI Security](https://aws.amazon.com/blogs/security/category/artificial-intelligence/generative-ai/)** – Guidance on secure AI architectures and compliance.
- **[Lakera AI Blog](https://www.lakera.ai/blog)** – Interactive campaigns and red teaming experiments.
- **[Securiti AI Security](https://securiti.ai/blog/)** – Articles on governance, risk, and compliance in the AI realm.
- **[PurpleSec AI & ML Security](https://purplesec.us/learn/ai-security/)** – Application of broad cybersecurity principles to AI/ML threat models.
- **[Wiz AI Security Articles](https://www.wiz.io/blog/top-10-ai-security-articles)** – Curated executive insights on AI risk trends.
- **[Lasso Security Blog](https://www.lasso.security/blog)** – In-depth research on prompt injection and adversarial LLM behavior.
- **[Cisco AI Safety](https://blogs.cisco.com/news/you-cant-sacrifice-ai-safety-for-ai-speed)** – Strategic perspectives on embedding safety in AI innovation.
- **[Microsoft Security: AI & ML](https://www.microsoft.com/en-us/security/blog/topic/ai-and-machine-learning/)** – Deep dives into threat modeling and responsible AI practices.
- **[Vectra AI Cybersecurity Blog](https://www.vectra.ai/blog)** – How AI can be used to defend against emerging AI-driven threats.
- **[Reversec AI Security Articles](https://labs.reversec.com/categories/ai-security)** – Reversec research articles on LLM application security and prompt injection  


## Communities

### Discord Communities
- **[LearnPrompting’s Prompt Hacking Discord](https://discord.com/channels/1046228027434086460/1349689482651369492)** – A hub for focused discussions on prompt hacking and red teaming.
- **[Pliny's BASI Discord](https://discord.com/channels/1105891499641684019/1235691879492751460)** – Covering behavioral AI safety and integrative research.
- **[AI Safety & Security Discord](https://discord.me/silicon-wall-e)** – A space for general discussions on AI risk, safety, and adversarial testing.
- **[AI Village Discord](https://aivillage.org/discord/)** – Associated with DEFCON’s AI Village, emphasizing practical red teaming.
- **[InfoSec Prep](https://discord.gg/infosecprep)** – For cybersecurity certifications with crossover AI security discussions.
- **[Hack The Box Discord](https://discord.gg/hackthebox)** – Active community blending traditional hacking with GenAI discussions.
- **[Laptop Hacking Coffee](https://discord.gg/lhc)** – Casual technical discussions on red teaming and ethical hacking.
- **[WhiteHat Security](https://discord.com/invite/whitehat-hacking-429657740562923521)** – Sharing insights on hacking and AI-enabled attacks.


### Reddit Communities
- **[ChatGPT Jailbreak Reddit](https://www.reddit.com/r/ChatGPTJailbreak/)** – Focus on testing limits of OpenAI’s models.
- **[ClaudeAI Jailbreak Reddit](https://www.reddit.com/r/ClaudeAIJailbreak/)** – Specifically for Anthropic’s Claude model vulnerabilities.
- **[NetSec Reddit](https://www.reddit.com/r/netsec/)** – Broader network security discussions that occasionally intersect with AI.
- **[Cybersecurity Reddit](https://www.reddit.com/r/cybersecurity/)** – General cybersecurity topics, including AI/ML threats.
- **[CybersecurityAI Reddit](https://www.reddit.com/r/cybersecurityAI/)** – Dedicated to AI-related cybersecurity challenges.
- **[Artificial Reddit](https://www.reddit.com/r/artificial/)** – Community discussions on AI safety, policy, and alignment.

## Courses

### Free Courses
- **[Introduction to Prompt Hacking](https://learnprompting.org/courses/intro-to-prompt-hacking)** – Beginner-focused course covering prompt injection fundamentals.
- **[Advanced Prompt Hacking](https://learnprompting.org/courses/advanced-prompt-hacking)** – Explores adversarial prompting and defense strategies in detail.
- **[Prompt Engineering for Beginners (DeepLearning.AI)](https://www.deeplearning.ai/short-courses/prompt-engineering-for-developers/)** – Effective prompt crafting using OpenAI models.
- **[Prompt Engineering Crash Course (DataCamp)](https://www.datacamp.com/courses/prompt-engineering-for-chatgpt)** – Hands-on training for prompt engineering with ChatGPT.
- **[Introduction to Prompt Engineering](https://learnprompting.org/courses/introduction_to_prompt_engineering)** – Techniques for writing optimized prompts.
- **[Intro to LLMs and Prompting (Google Cloud)](https://www.cloudskillsboost.google/paths/118)** – Overview of LLM concepts and practical prompting within a cloud framework.
- **[Prompt Engineering on LearnAI](https://learnprompting.org/)** – Community-driven prompt engineering resources.
- **[Generative AI Prompting Basics (Google)](https://cloud.google.com/training/courses/generative-ai-prompting)** – Foundational course for generative AI prompting.
- **[Prompt Engineering on Fast.ai](https://course.fast.ai/)** – Integrated with broader practical machine learning applications.
- **[Prompt Engineering Guide (GitHub)](https://github.com/dair-ai/Prompt-Engineering-Guide)** – An open-source resource for advanced prompt design.
- **[Intro to AI Safety and Prompt Testing](https://www.eleuther.ai/)** – Materials focused on the safety aspects of prompt exploitation.
- **[Spikee Tutorial Series](https://www.youtube.com/playlist?list=PLNg09XqZv0dEeneAyDR4nxPda8WJBOKAe)** – Series of tutorials on how to use spikee, an open-source tool, to test LLMs and LLM application resiliance to different jailbreaking and prompt injection attacks 

### Paid Courses
- **[AI Red-Teaming and Security Masterclass](https://learnprompting.org/courses/ai-security-masterclass)** – Comprehensive training on threat assessment, red teaming methodologies, and effective countermeasures.
- **[Attacking AI](https://payhip.com/b/xysOk)** – An advanced course focusing on offensive AI security and adversarial strategies.

# Events

Keep updated with competitions, workshops, and summits that drive practical learning and networking:
- **[HackAPrompt](https://www.hackaprompt.com/)** – Online competitions aimed at discovering innovative ways to jailbreak AI systems.
- **[RedTeam Arena](https://redarena.ai/)** – A gamified platform to identify and exploit LLM vulnerabilities.
- **[AI Security Summit 2024](https://www.scale.com/summit/access)** – Executive-level summit addressing current AI security challenges.
- **[AI Red-Teaming Workshop (SEI)](https://insights.sei.cmu.edu/news/ai-red-teaming-workshop-will-explore-best-practices/)** – Workshops exploring advanced red teaming techniques.
- **[AISec Workshop](https://aisec.cc/)** – Academic insights held alongside major ML conferences.
- **[AI Security Symposium 2024](https://info.checkmarx.com/ai-security-symposium-2024)** – Sessions on risk mitigation and AI adoption strategies.
- **[Black Hat USA 2024 AI Summit](https://www.blackhat.com/us-24/ai-summit.html)** – Practical sessions on LLM vulnerabilities integrated in the Black Hat conference.
- **[AI Cybersecurity Summit 2025 (SANS)](https://www.sans.org/cyber-security-training-events/ai-summit-2025/)** – Technical labs and sessions bridging AI and cybersecurity.
- **[Generative AI Red Teaming Challenge 2024 (Clova)](https://clova.ai/en/tech-blog/en-generative-ai-red-teaming-challenge-2024)** – Competitive event to stress-test and enhance the resilience of LLMs.

## Jailbreaks

A collection of repositories, tools, and research papers that document methods of bypassing LLM safeguards:
- **[L1B3RT4S](https://github.com/elder-plinius/L1B3RT4S/tree/main)** – Repository featuring various jailbreak prompt sets and evaluation tools.
- **[Jailbreak Tracker](https://jailbreak-tracker-goochbeaterhs.replit.app/)** – Live dashboard for monitoring known jailbreak prompts.
- **[Awesome GPT Super Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting)** – Curated list of red teaming and jailbreak resources for GPT-based models.
- **[Jailbreaking in GenAI: Techniques and Ethical Implications](https://learnprompting.org/docs/prompt_hacking/jailbreaking)** – Guide discussing the practical methods and ethical considerations.
- **[Jailbreaking LLMs: A Comprehensive Guide (With Examples)](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/)** – Step-by-step guide showcasing real-world examples.
- **[AI Jailbreak – IBM](https://www.ibm.com/think/insights/ai-jailbreak)** – Overview of jailbreak risks and mitigation strategies.
- **[AI Jailbreaking Demo: How Prompt Engineering Bypasses LLM Security Measures](https://www.youtube.com/watch?v=F_KychntktU)** – Video walkthrough highlighting bypass techniques.
- **[Prompt Injection vs. Jailbreaking: What’s the Difference?](https://learnprompting.org/blog/injection_jailbreaking)** – Comparative discussion on prompt injection and jailbreak strategies.
- **[GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253)** – Research outlining an automated framework for generating jailbreak prompts.
- **[DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522)** – Novel approach applying diffusion models in jailbreak generation.
- **[SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902)** – Framework that leverages social engineering concepts for jailbreaks.
- **[Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317)** – Technique to boost jailbreak effectiveness using adversarial translation.
- **[AI Jailbreaks: What They Are and How They Can Be Mitigated](https://www.ibm.com/think/insights/ai-jailbreak)** – Additional insights into mitigating jailbreak vulnerabilities.

## YouTube

### AI Red Teaming
- **[How Microsoft Approaches AI Red Teaming](https://www.youtube.com/watch?v=zFRn_RMSPI4)** – Insight into Microsoft's methodologies.
- **[AI Red Teaming in 2024 and Beyond](https://www.youtube.com/watch?v=nzfPUeB6UjM)** – Discussion of emerging trends.
- **[Red Teaming AI: What You Need To Know](https://www.youtube.com/watch?v=2WvxYDpXw5s)** – Essential introduction to red teaming practices.
- **[Building Trust in AI: Introduction to Red-Teaming](https://www.youtube.com/watch?v=Zw_ulylWrhs)** – Fundamentals of effective AI red teaming.
- **[What's Next for AI Red-Teaming?](https://www.youtube.com/watch?v=gDnNuxpvPis)** – Future directions and challenges.
- **[LLM Security Chronicles](https://www.youtube.com/playlist?list=PLNg09XqZv0dHVDw7OiiRQJ315HnGHbDb)** – YouTube playlist on practical LLM application security and red teaming  

### Jailbreaking
- **[How AI Jailbreaks Work and What Stops Them?](https://www.youtube.com/watch?v=6Mmevs1877A)** – An overview of the methods behind AI jailbreaks.
- **[AI Jailbreaking Demo](https://www.youtube.com/watch?v=F_KychntktU)** – Demonstration of bypass techniques in practice.
- **[How Jailbreakers Try to “Free” AI](https://www.youtube.com/watch?v=CIQe2jdYAJ0)** – Examines the mindset behind prompt injection attacks.
- **[Defending Against AI Jailbreaks](https://www.youtube.com/watch?v=BaNXYqcfDyo)** – Strategies for protecting LLMs from adversarial prompts.
- **[AI Jailbroken in 30 Seconds?!](https://www.youtube.com/watch?v=YatNUON2yOQ)** – A demonstration of rapid prompt injection.
- **[Anthropic's Stunning New Jailbreak](https://www.youtube.com/watch?v=LGHaMcP_flA)** – Review of a breakthrough jailbreak technique.
- **[New AI Jailbreak Method Shatters Models](https://www.youtube.com/watch?v=5cEvNO9rZgI)** – Exploration of experimental bypass strategies.
- **[Jailbreaking AI - Deepseek & Prompt Tricks](https://www.youtube.com/watch?v=9TVG9Oxda0M)** – Advanced prompt engineering in real-world scenarios.
- **[First to Jailbreak Claude Wins $20,000](https://www.youtube.com/watch?v=m5uWKRJhcao)** – Competitive challenge highlighting effective prompt injection.

---
## Contributing

If you have suggestions, improvements, or additional resources to include, please review our [CONTRIBUTING.md](CONTRIBUTING.md) guidelines and submit a pull request.

---

This repository aims to deliver critical, reliable resources for advancing prompt hacking research. We encourage rigorous testing, honest discussions, and the sharing of proven methodologies to foster safe and responsible exploration in this field.
