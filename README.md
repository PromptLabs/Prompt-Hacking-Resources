      
<h1 align="center">
    <strong>Awesome prompt hacking ‚Äì an awesome list of curated resources for people interested in AI Red Teaming, Jailbreaking, and Prompt Injection.</strong>
</h1>
<p align="center">

Prompt Hacking is an emerging field that covers the intersection between AI and Cybersecurity. Due to its novelty, online resources are few and far between.

This repo aims to provide a good overview of resources people can use to upskill themselves at Prompt Hacking.

  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Made With Love](https://img.shields.io/badge/Made%20With-Love-orange.svg)](https://github.com/chetanraj/awesome-github-badges) [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Awesome%20Prompting%20Resources%20-%20a%20collection%20of%20awesome%20lists%20related%20to%20prompt%20engineering%20by%20@learnprompting&url=https://github.com/kavaivaleri/prompting-resources)
</p>

> This resource is provided by [Learn Prompting](https://github.com/trigaten/Learn_Prompting), your go-to resource for mastering Generative AI.

<img src="astronaut-learn-prompting.png" alt="astronaut-learn-prompting">

<p align="center">
<a href="https://discord.com/invite/learn-prompting-1046228027434086460">Discord</a> ‚Ä¢ <a href="https://x.com/learnprompting">Twitter (X)</a> ‚Ä¢ <a href="https://www.linkedin.com/company/learn-prompting/">LinkedIn</a> ‚Ä¢ <a href="https://learnprompting.beehiiv.com/subscribe">Newsletter</a> ‚Ä¢ <a href="https://learnprompting.org/courses/chatgpt-for-everyone">Free ChatGPT Course</a> ‚Ä¢ <a href="https://learnprompting.org/docs/introduction">Free Prompt Engineering Guide</a>

</p>

## üóÇÔ∏è Resource Categories

- **[Blogs](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/BLOGS.md)** - Written content covering core concepts and novel research
- **[Communities](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/COMMUNITIES.md)** - Places you can hang out and discuss Prompt Hacking
- **[Courses](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/COURSES.md)** - Structured learning paths covering AI Security content 
- **[Events](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/EVENTS.md)** - Test your skills in paid competitions
- **[Jailbreaks](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/JAILBREAKS.md)** - Look at other people's techniques for Hacking LLMs
- **[YouTube](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/YOUTUBE.md)** - Video content including tutorials, breakdowns, and real-world red teaming walkthroughs

We hope you find this useful. If you have any suggestions, please let us know.

## Blogs

‚Ä¢ [InjectPrompt](https://injectprompt.com) ‚Äì List of novel jailbreaks, prompt injections, and system prompt leaks  
‚Ä¢ [LearnPrompting Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/introduction) ‚Äì Step-by-step educational guide to prompt injection and model exploitation  
‚Ä¢ [AIBlade](https://www.aiblade.net/) ‚Äì Curated directory of AI red teaming tools and resources    
‚Ä¢ [EmbraceTheRed](https://embracethered.com/blog/) ‚Äì Practical experiments and insights from active AI red teamers  
‚Ä¢ [Joseph Thacker](https://josephthacker.com/) ‚Äì First-person red teaming explorations and LLM vulnerability research  
‚Ä¢ [Protect AI Blog](https://protectai.com/blog) ‚Äì Enterprise-grade AI security insights and open-source tooling announcements  
‚Ä¢ [AWS Generative AI Security](https://aws.amazon.com/blogs/security/category/artificial-intelligence/generative-ai/) ‚Äì Secure architecture and compliance guidance for GenAI workloads  
‚Ä¢ [Lakera AI Blog](https://www.lakera.ai/blog) ‚Äì Interactive red teaming campaigns and accessible safety content (e.g., Gandalf)  
‚Ä¢ [Securiti AI Security](https://securiti.ai/blog/) ‚Äì Governance, risk, and compliance content for data-centric AI security  
‚Ä¢ [PurpleSec AI & ML Security](https://purplesec.us/learn/ai-security/) ‚Äì Broad cybersecurity context applied to AI/ML threat models  
‚Ä¢ [Wiz AI Security Articles](https://www.wiz.io/blog/top-10-ai-security-articles) ‚Äì Curated executive-level insights on AI risk and security trends  
‚Ä¢ [Lasso Security Blog](https://www.lasso.security/blog) ‚Äì Offensive research into prompt injection, training set leaks, and adversarial LLM behavior  
‚Ä¢ [Cisco AI Safety](https://blogs.cisco.com/news/you-cant-sacrifice-ai-safety-for-ai-speed) ‚Äì Strategic perspective on embedding AI safety into innovation cycles  
‚Ä¢ [Microsoft Security: AI & ML](https://www.microsoft.com/en-us/security/blog/topic/ai-and-machine-learning/) ‚Äì Deep dives into red teaming, threat modeling, and Responsible AI practices  
‚Ä¢ [Vectra AI Cybersecurity Blog](https://www.vectra.ai/blog) ‚Äì Using AI to defend against AI-driven threats in enterprise security

## Communities

### Discord Communities
‚Ä¢ [LearnPrompting's Prompt Hacking Discord](https://discord.com/channels/1046228027434086460/1349689482651369492) ‚Äì Community focused on prompt hacking and AI red teaming education  
‚Ä¢ [Pliny's BASI Discord](https://discord.com/channels/1105891499641684019/1235691879492751460) ‚Äì Discussion and research on behavioral AI safety and integrity (BASI)  
‚Ä¢ [AI Safety & Security Discord](https://discord.me/silicon-wall-e) ‚Äì General community around AI risk, safety, and adversarial testing  
‚Ä¢ [AI Village Discord](https://aivillage.org/discord/) ‚Äì Linked to DEFCON‚Äôs AI Village; focused on red teaming, hacking, and AI security  
‚Ä¢ [InfoSec Prep](https://discord.gg/infosecprep) ‚Äì Support and resources for cybersecurity certs, with some AI security crossover  
‚Ä¢ [Hack The Box Discord](https://discord.gg/hackthebox) ‚Äì Active hacking and cybersecurity hub with GenAI discussion channels  
‚Ä¢ [Laptop Hacking Coffee](https://discord.gg/lhc) ‚Äì Chill and technical space for infosec, red teaming, and ethical hacking  
‚Ä¢ [WhiteHat Security](https://discord.com/invite/whitehat-hacking-429657740562923521) ‚Äì Hacking and security knowledge sharing, with active discussions on AI-enabled attacks


### Reddit Communities
‚Ä¢ [ChatGPT Jailbreak Reddit](https://www.reddit.com/r/ChatGPTJailbreak/) ‚Äì Community focused on testing the limits and vulnerabilities of OpenAI‚Äôs models  
‚Ä¢ [ClaudeAI Jailbreak Reddit](https://www.reddit.com/r/ClaudeAIJailbreak/) ‚Äì Similar to above, centered on Anthropic‚Äôs Claude model  
‚Ä¢ [NetSec Reddit](https://www.reddit.com/r/netsec/) ‚Äì General network security subreddit, often featuring AI-related threat vectors  
‚Ä¢ [Cybersecurity Reddit](https://www.reddit.com/r/cybersecurity/) ‚Äì Broad industry trends and community insights, including AI and ML  
‚Ä¢ [CybersecurityAI Reddit](https://www.reddit.com/r/cybersecurityAI/) ‚Äì Specifically focused on AI-related threats, defenses, and security tooling  
‚Ä¢ [Artificial Reddit](https://www.reddit.com/r/artificial/) ‚Äì General AI discussion, including safety, policy, and LLM alignment threads

## Courses

### Free Courses
‚Ä¢ [Introduction to Prompt Hacking](https://learnprompting.org/courses/intro-to-prompt-hacking) ‚Äì Beginner-friendly course covering the fundamentals of prompt injection and AI red teaming  
‚Ä¢ [Advanced Prompt Hacking](https://learnprompting.org/courses/advanced-prompt-hacking) ‚Äì Deeper dive into adversarial prompting, attack types, and defense strategies  
‚Ä¢ [Prompt Engineering for Beginners (DeepLearning.AI)](https://www.deeplearning.ai/short-courses/prompt-engineering-for-developers/) ‚Äì Short course on crafting effective prompts using OpenAI models  
‚Ä¢ [Prompt Engineering Crash Course (DataCamp)](https://www.datacamp.com/courses/prompt-engineering-for-chatgpt) ‚Äì Hands-on training on prompt engineering with ChatGPT   
‚Ä¢ [Introduction to Prompt Engineering](https://learnprompting.org/courses/introduction_to_prompt_engineering) ‚Äì Write better prompts and learn how LLMs interpret inputs  
‚Ä¢ [Intro to LLMs and Prompting (Google Cloud)](https://www.cloudskillsboost.google/paths/118) ‚Äì Google‚Äôs path on LLM basics and prompting within their cloud platform  
‚Ä¢ [Prompt Engineering on LearnAI](https://learnprompting.org/) ‚Äì Community-driven learning resources for prompt engineering and red teaming  
‚Ä¢ [Generative AI Prompting Basics (Google)](https://cloud.google.com/training/courses/generative-ai-prompting) ‚Äì Foundational course for understanding GenAI prompting with Google tools  
‚Ä¢ [Prompt Engineering on Fast.ai](https://course.fast.ai/) ‚Äì Included in Fast.ai‚Äôs broader course, focusing on real-world prompting and LLM use cases  
‚Ä¢ [Prompt Engineering Guide (GitHub)](https://github.com/dair-ai/Prompt-Engineering-Guide) ‚Äì Open-source, curated guide for learning prompt design and applications  
‚Ä¢ [Intro to AI Safety and Prompt Testing](https://www.eleuther.ai/) ‚Äì Educational resources and reading materials from EleutherAI‚Äôs safety efforts


### Paid Courses
‚Ä¢ [AI Red-Teaming and Security Masterclass](https://learnprompting.org/courses/ai-security-masterclass) ‚Äì Comprehensive training on AI red teaming, threats, testing methods, and tools  
‚Ä¢ [Attacking AI](https://payhip.com/b/xysOk) ‚Äì Advanced course on offensive AI security and real-world adversarial techniques

# Events

‚Ä¢ [HackAPrompt](https://www.hackaprompt.com/) ‚Äì Online competition where participants try to jailbreak AI systems through adversarial prompt crafting  
‚Ä¢ [RedTeam Arena](https://redarena.ai/) ‚Äì Gamified AI red teaming platform focused on discovering vulnerabilities in LLMs  
‚Ä¢ [AI Security Summit 2024](https://www.scale.com/summit/access) ‚Äì Executive-level summit by Scale AI, addressing the latest developments in AI security and safety  
‚Ä¢ [AI Red-Teaming Workshop (SEI)](https://insights.sei.cmu.edu/news/ai-red-teaming-workshop-will-explore-best-practices/) ‚Äì Workshop by CMU SEI focused on methodologies and best practices in red-teaming AI systems  
‚Ä¢ [AISec Workshop](https://aisec.cc/) ‚Äì Academic workshop co-located with major ML conferences (like CCS/NeurIPS) on AI security and privacy research  
‚Ä¢ [AI Security Symposium 2024](https://info.checkmarx.com/ai-security-symposium-2024) ‚Äì Event focused on the risks and strategies around secure AI adoption in enterprise environments  
‚Ä¢ [Black Hat USA 2024 AI Summit](https://www.blackhat.com/us-24/ai-summit.html) ‚Äì Part of Black Hat USA, featuring talks on LLM security, adversarial ML, and real-world red teaming   
‚Ä¢ [AI Cybersecurity Summit 2025 (SANS)](https://www.sans.org/cyber-security-training-events/ai-summit-2025/) ‚Äì Summit offering technical sessions and hands-on labs at the intersection of cybersecurity and AI  
‚Ä¢ [Generative AI Red Teaming Challenge 2024 (Clova)](https://clova.ai/en/tech-blog/en-generative-ai-red-teaming-challenge-2024) ‚Äì Competitive red teaming event by Clova to stress-test and harden LLMs

## Jailbreaks

‚Ä¢ [L1B3RT4S](https://github.com/elder-plinius/L1B3RT4S/tree/main) ‚Äì A GitHub repository with jailbreak prompt sets and tools for evaluating LLM security  
‚Ä¢ [Jailbreak Tracker](https://jailbreak-tracker-goochbeaterhs.replit.app/) ‚Äì Live dashboard tracking known jailbreak prompts across different LLMs  
‚Ä¢ [Awesome GPT Super Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting) ‚Äì Curated list of red teaming and jailbreak resources for GPT-style models  
‚Ä¢ [Jailbreaking in GenAI: Techniques and Ethical Implications](https://learnprompting.org/docs/prompt_hacking/jailbreaking) ‚Äì Explores jailbreak methods alongside their ethical and societal concerns  
‚Ä¢ [Jailbreaking LLMs: A Comprehensive Guide (With Examples)](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/) ‚Äì Practical guide offering step-by-step jailbreak prompt examples and techniques  
‚Ä¢ [AI Jailbreak ‚Äì IBM](https://www.ibm.com/think/insights/ai-jailbreak) ‚Äì Overview of jailbreak threats, implications, and potential mitigation strategies  
‚Ä¢ [AI Jailbreaking Demo: How Prompt Engineering Bypasses LLM Security Measures](https://www.youtube.com/watch?v=F_KychntktU) ‚Äì Video walkthrough demonstrating how prompt engineering can bypass model restrictions  
‚Ä¢ [Prompt Injection vs. Jailbreaking: What's the Difference?](https://learnprompting.org/blog/injection_jailbreaking) ‚Äì Comparison of two related AI exploitation methods: prompt injection vs jailbreaks  
‚Ä¢ [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253) ‚Äì Research paper presenting a framework to auto-generate jailbreak prompts for security testing  
‚Ä¢ [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522) ‚Äì Novel method using diffusion models to create jailbreak prompts for large language models  
‚Ä¢ [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902) ‚Äì Proposes a jailbreak generation framework leveraging social engineering concepts  
‚Ä¢ [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317) ‚Äì Introduces techniques to improve jailbreak effectiveness through prompt translation methods  
‚Ä¢ [AI Jailbreaks: What They Are and How They Can Be Mitigated](https://www.ibm.com/think/insights/ai-jailbreak) ‚Äì Additional IBM post explaining jailbreak risks and corporate security approaches

## YouTube

### AI Red Teaming
- [How Microsoft Approaches AI Red Teaming](https://www.youtube.com/watch?v=zFRn_RMSPI4) ‚Äì Insights into Microsoft's AI red teaming strategies  
- [AI Red Teaming in 2024 and Beyond](https://www.youtube.com/watch?v=nzfPUeB6UjM) ‚Äì Exploration of red teaming trends and tools  
- [Red Teaming AI: What You Need To Know](https://www.youtube.com/watch?v=2WvxYDpXw5s) ‚Äì Comprehensive overview of red teaming essentials  
- [Building Trust in AI: Introduction to Red-Teaming](https://www.youtube.com/watch?v=Zw_ulylWrhs) ‚Äì Fundamentals of red-teaming for AI  
- [What's Next for AI Red-Teaming?](https://www.youtube.com/watch?v=gDnNuxpvPis) ‚Äì Future challenges and developments in the field  

### Jailbreaking
- [How AI Jailbreaks Work and What Stops Them?](https://www.youtube.com/watch?v=6Mmevs1877A) ‚Äì How jailbreaks are done and defended  
- [AI Jailbreaking Demo](https://www.youtube.com/watch?v=F_KychntktU) ‚Äì How prompt engineering bypasses LLM filters  
- [How Jailbreakers Try to ‚ÄúFree‚Äù AI](https://www.youtube.com/watch?v=CIQe2jdYAJ0) ‚Äì The mindset behind jailbreakers  
- [Defending Against AI Jailbreaks](https://www.youtube.com/watch?v=BaNXYqcfDyo) ‚Äì Protection strategies  
- [AI Jailbroken in 30 Seconds?!](https://www.youtube.com/watch?v=YatNUON2yOQ) ‚Äì How fast prompt injection can occur  
- [Anthropic's Stunning New Jailbreak](https://www.youtube.com/watch?v=LGHaMcP_flA) ‚Äì Includes prompt injection method  
- [New AI Jailbreak Method Shatters Models](https://www.youtube.com/watch?v=5cEvNO9rZgI) ‚Äì Attack working on GPT-4 and others  
- [Jailbreaking AI - Deepseek & Prompt Tricks](https://www.youtube.com/watch?v=9TVG9Oxda0M) ‚Äì Advanced prompt injection in the wild  
- [First to Jailbreak Claude Wins $20,000](https://www.youtube.com/watch?v=m5uWKRJhcao) ‚Äì Real challenge involving prompt injection  
