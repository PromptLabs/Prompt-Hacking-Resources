# Prompt-Hacking-Resources
 **A list of curated resources for people interested in AI Red Teaming, Jailbreaking, and Prompt Injection**

 Prompt Hacking is an emerging field that covers the intersection between AI and Cybersecurity. Due to its novelty, online resources are few and far between.

 This repo aims to provide a good overview of resources people can use to upskill themselves at Prompt Hacking.

 ## Sections

- **[BLOGS](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/BLOGS.md)** - Written content covering core concepts and novel research
- **[COMMUNITIES](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/COMMUNITIES.md)** - Places you can hang out and discuss Prompt Hacking
- **[COURSES](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/COURSES.md)** - Structured learning paths covering AI Security content 
- **[EVENTS](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/EVENTS.md)** - Test your skills in paid competitions
- **[JAILBREAKS](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/JAILBREAKS.md)** - Look at other people's techniques for Hacking LLMs
- **[YOUTUBE](https://github.com/PromptLabs/Prompt-Hacking-Resources/blob/main/YOUTUBE.mkd)** - Video content including tutorials, breakdowns, and real-world red teaming walkthroughs

We hope you find this useful. Any suggestions, please let us know.

---
# BLOGS

• [InjectPrompt](https://injectprompt.com) – List of novel jailbreaks, prompt injections, and system prompt leaks  
• [LearnPrompting Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/introduction) – Step-by-step educational guide to prompt injection and model exploitation  
• [AIBlade](https://www.aiblade.net/) – Curated directory of AI red teaming tools and resources    
• [EmbraceTheRed](https://embracethered.com/blog/) – Practical experiments and insights from active AI red teamers  
• [Joseph Thacker](https://josephthacker.com/) – First-person red teaming explorations and LLM vulnerability research  
• [Protect AI Blog](https://protectai.com/blog) – Enterprise-grade AI security insights and open-source tooling announcements  
• [AWS Generative AI Security](https://aws.amazon.com/blogs/security/category/artificial-intelligence/generative-ai/) – Secure architecture and compliance guidance for GenAI workloads  
• [Lakera AI Blog](https://www.lakera.ai/blog) – Interactive red teaming campaigns and accessible safety content (e.g., Gandalf)  
• [Securiti AI Security](https://securiti.ai/blog/) – Governance, risk, and compliance content for data-centric AI security  
• [PurpleSec AI & ML Security](https://purplesec.us/learn/ai-security/) – Broad cybersecurity context applied to AI/ML threat models  
• [Wiz AI Security Articles](https://www.wiz.io/blog/top-10-ai-security-articles) – Curated executive-level insights on AI risk and security trends  
• [Lasso Security Blog](https://www.lasso.security/blog) – Offensive research into prompt injection, training set leaks, and adversarial LLM behavior  
• [Cisco AI Safety](https://blogs.cisco.com/news/you-cant-sacrifice-ai-safety-for-ai-speed) – Strategic perspective on embedding AI safety into innovation cycles  
• [Microsoft Security: AI & ML](https://www.microsoft.com/en-us/security/blog/topic/ai-and-machine-learning/) – Deep dives into red teaming, threat modeling, and Responsible AI practices  
• [Vectra AI Cybersecurity Blog](https://www.vectra.ai/blog) – Using AI to defend against AI-driven threats in enterprise security

---
# COMMUNITIES

**Discord Communities**  
• [LearnPrompting's Prompt Hacking Discord](https://discord.com/channels/1046228027434086460/1349689482651369492) – Community focused on prompt hacking and AI red teaming education  
• [Pliny's BASI Discord](https://discord.com/channels/1105891499641684019/1235691879492751460) – Discussion and research on behavioral AI safety and integrity (BASI)  
• [AI Safety & Security Discord](https://discord.me/silicon-wall-e) – General community around AI risk, safety, and adversarial testing  
• [AI Village Discord](https://aivillage.org/discord/) – Linked to DEFCON’s AI Village; focused on red teaming, hacking, and AI security  
• [InfoSec Prep](https://discord.gg/infosecprep) – Support and resources for cybersecurity certs, with some AI security crossover  
• [Hack The Box Discord](https://discord.gg/hackthebox) – Active hacking and cybersecurity hub with GenAI discussion channels  
• [Laptop Hacking Coffee](https://discord.gg/lhc) – Chill and technical space for infosec, red teaming, and ethical hacking  
• [WhiteHat Security](https://discord.com/invite/whitehat-hacking-429657740562923521) – Hacking and security knowledge sharing, with active discussions on AI-enabled attacks


**Reddit Communities**  
• [ChatGPT Jailbreak Reddit](https://www.reddit.com/r/ChatGPTJailbreak/) – Community focused on testing the limits and vulnerabilities of OpenAI’s models  
• [ClaudeAI Jailbreak Reddit](https://www.reddit.com/r/ClaudeAIJailbreak/) – Similar to above, centered on Anthropic’s Claude model  
• [NetSec Reddit](https://www.reddit.com/r/netsec/) – General network security subreddit, often featuring AI-related threat vectors  
• [Cybersecurity Reddit](https://www.reddit.com/r/cybersecurity/) – Broad industry trends and community insights, including AI and ML  
• [CybersecurityAI Reddit](https://www.reddit.com/r/cybersecurityAI/) – Specifically focused on AI-related threats, defenses, and security tooling  
• [Artificial Reddit](https://www.reddit.com/r/artificial/) – General AI discussion, including safety, policy, and LLM alignment threads

---
# COURSES

**Free Courses**  
• [Introduction to Prompt Hacking](https://learnprompting.org/courses/intro-to-prompt-hacking) – Beginner-friendly course covering the fundamentals of prompt injection and AI red teaming  
• [Advanced Prompt Hacking](https://learnprompting.org/courses/advanced-prompt-hacking) – Deeper dive into adversarial prompting, attack types, and defense strategies  
• [Prompt Engineering for Beginners (DeepLearning.AI)](https://www.deeplearning.ai/short-courses/prompt-engineering-for-developers/) – Short course on crafting effective prompts using OpenAI models  
• [Prompt Engineering Crash Course (DataCamp)](https://www.datacamp.com/courses/prompt-engineering-for-chatgpt) – Hands-on training on prompt engineering with ChatGPT   
• [Introduction to Prompt Engineering](https://learnprompting.org/courses/introduction_to_prompt_engineering) – Write better prompts and learn how LLMs interpret inputs  
• [Intro to LLMs and Prompting (Google Cloud)](https://www.cloudskillsboost.google/paths/118) – Google’s path on LLM basics and prompting within their cloud platform  
• [Prompt Engineering on LearnAI](https://learnprompting.org/) – Community-driven learning resources for prompt engineering and red teaming  
• [Generative AI Prompting Basics (Google)](https://cloud.google.com/training/courses/generative-ai-prompting) – Foundational course for understanding GenAI prompting with Google tools  
• [Prompt Engineering on Fast.ai](https://course.fast.ai/) – Included in Fast.ai’s broader course, focusing on real-world prompting and LLM use cases  
• [Prompt Engineering Guide (GitHub)](https://github.com/dair-ai/Prompt-Engineering-Guide) – Open-source, curated guide for learning prompt design and applications  
• [Intro to AI Safety and Prompt Testing](https://www.eleuther.ai/) – Educational resources and reading materials from EleutherAI’s safety efforts


**Paid Courses**  
• [AI Red-Teaming and Security Masterclass](https://learnprompting.org/courses/ai-security-masterclass) – Comprehensive training on AI red teaming, threats, testing methods, and tools  
• [Attacking AI](https://payhip.com/b/xysOk) – Advanced course on offensive AI security and real-world adversarial techniques

---
# EVENTS

• [HackAPrompt](https://www.hackaprompt.com/) – Online competition where participants try to jailbreak AI systems through adversarial prompt crafting  
• [RedTeam Arena](https://redarena.ai/) – Gamified AI red teaming platform focused on discovering vulnerabilities in LLMs  
• [AI Security Summit 2024](https://www.scale.com/summit/access) – Executive-level summit by Scale AI, addressing the latest developments in AI security and safety  
• [AI Red-Teaming Workshop (SEI)](https://insights.sei.cmu.edu/news/ai-red-teaming-workshop-will-explore-best-practices/) – Workshop by CMU SEI focused on methodologies and best practices in red-teaming AI systems  
• [AISec Workshop](https://aisec.cc/) – Academic workshop co-located with major ML conferences (like CCS/NeurIPS) on AI security and privacy research  
• [AI Security Symposium 2024](https://info.checkmarx.com/ai-security-symposium-2024) – Event focused on the risks and strategies around secure AI adoption in enterprise environments  
• [Black Hat USA 2024 AI Summit](https://www.blackhat.com/us-24/ai-summit.html) – Part of Black Hat USA, featuring talks on LLM security, adversarial ML, and real-world red teaming   
• [AI Cybersecurity Summit 2025 (SANS)](https://www.sans.org/cyber-security-training-events/ai-summit-2025/) – Summit offering technical sessions and hands-on labs at the intersection of cybersecurity and AI  
• [Generative AI Red Teaming Challenge 2024 (Clova)](https://clova.ai/en/tech-blog/en-generative-ai-red-teaming-challenge-2024) – Competitive red teaming event by Clova to stress-test and harden LLMs

---
# JAILBREAKS

• [L1B3RT4S](https://github.com/elder-plinius/L1B3RT4S/tree/main) – A GitHub repository with jailbreak prompt sets and tools for evaluating LLM security  
• [Jailbreak Tracker](https://jailbreak-tracker-goochbeaterhs.replit.app/) – Live dashboard tracking known jailbreak prompts across different LLMs  
• [Awesome GPT Super Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting) – Curated list of red teaming and jailbreak resources for GPT-style models  
• [Jailbreaking in GenAI: Techniques and Ethical Implications](https://learnprompting.org/docs/prompt_hacking/jailbreaking) – Explores jailbreak methods alongside their ethical and societal concerns  
• [Jailbreaking LLMs: A Comprehensive Guide (With Examples)](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/) – Practical guide offering step-by-step jailbreak prompt examples and techniques  
• [AI Jailbreak – IBM](https://www.ibm.com/think/insights/ai-jailbreak) – Overview of jailbreak threats, implications, and potential mitigation strategies  
• [AI Jailbreaking Demo: How Prompt Engineering Bypasses LLM Security Measures](https://www.youtube.com/watch?v=F_KychntktU) – Video walkthrough demonstrating how prompt engineering can bypass model restrictions  
• [Prompt Injection vs. Jailbreaking: What's the Difference?](https://learnprompting.org/blog/injection_jailbreaking) – Comparison of two related AI exploitation methods: prompt injection vs jailbreaks  
• [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253) – Research paper presenting a framework to auto-generate jailbreak prompts for security testing  
• [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522) – Novel method using diffusion models to create jailbreak prompts for large language models  
• [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902) – Proposes a jailbreak generation framework leveraging social engineering concepts  
• [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317) – Introduces techniques to improve jailbreak effectiveness through prompt translation methods  
• [AI Jailbreaks: What They Are and How They Can Be Mitigated](https://www.ibm.com/think/insights/ai-jailbreak) – Additional IBM post explaining jailbreak risks and corporate security approaches

---
# YouTube

**AI Red Teaming**
- [How Microsoft Approaches AI Red Teaming](https://www.youtube.com/watch?v=zFRn_RMSPI4) – Insights into Microsoft's AI red teaming strategies  
- [AI Red Teaming in 2024 and Beyond](https://www.youtube.com/watch?v=nzfPUeB6UjM) – Exploration of red teaming trends and tools  
- [Red Teaming AI: What You Need To Know](https://www.youtube.com/watch?v=2WvxYDpXw5s) – Comprehensive overview of red teaming essentials  
- [Building Trust in AI: Introduction to Red-Teaming](https://www.youtube.com/watch?v=Zw_ulylWrhs) – Fundamentals of red-teaming for AI  
- [What's Next for AI Red-Teaming?](https://www.youtube.com/watch?v=gDnNuxpvPis) – Future challenges and developments in the field  

**Jailbreaking**
- [How AI Jailbreaks Work and What Stops Them?](https://www.youtube.com/watch?v=6Mmevs1877A) – How jailbreaks are done and defended  
- [AI Jailbreaking Demo](https://www.youtube.com/watch?v=F_KychntktU) – How prompt engineering bypasses LLM filters  
- [How Jailbreakers Try to “Free” AI](https://www.youtube.com/watch?v=CIQe2jdYAJ0) – The mindset behind jailbreakers  
- [Defending Against AI Jailbreaks](https://www.youtube.com/watch?v=BaNXYqcfDyo) – Protection strategies  
- [AI Jailbroken in 30 Seconds?!](https://www.youtube.com/watch?v=YatNUON2yOQ) – How fast prompt injection can occur  
- [Anthropic's Stunning New Jailbreak](https://www.youtube.com/watch?v=LGHaMcP_flA) – Includes prompt injection method  
- [New AI Jailbreak Method Shatters Models](https://www.youtube.com/watch?v=5cEvNO9rZgI) – Attack working on GPT-4 and others  
- [Jailbreaking AI - Deepseek & Prompt Tricks](https://www.youtube.com/watch?v=9TVG9Oxda0M) – Advanced prompt injection in the wild  
- [First to Jailbreak Claude Wins $20,000](https://www.youtube.com/watch?v=m5uWKRJhcao) – Real challenge involving prompt injection  
